{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from autocorrect import spell\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import itertools, nltk, re, pprint, string, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "    \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "   \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "    \"VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "   \"The book was good.\",         # positive sentence\n",
    "   \"The book was kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "  \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "   \"A really bad, horrible book.\",       # negative sentence with booster words\n",
    "   \"At least it isn't a horrible book.\", # negated negative sentence with contraction\n",
    "    \":) and :D\",     # emoticons handled\n",
    "    \"\",              # an empty string is correctly handled\n",
    "    \"Today sux\",     #  negative slang handled\n",
    "    \"Today sux!\",    #  negative slang with punctuation emphasis handled\n",
    "    \"Today SUX!\",    #  negative slang with capitalization emphasis\n",
    "   \"Today kinda sux! But I'll get by, lol\" # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)\n",
    "#     ss = sid.polarity_scores(sentence)\n",
    "#     for k in sorted(ss):\n",
    "#          print('{0}: {1}, '.format(k, ss[k]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = 'OMG!! such a horrible thing.'\n",
    "sid.polarity_scores(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.5423, 'neg': 0.778, 'neu': 0.222, 'pos': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'NOT BAD.'\n",
    "sid.polarity_scores(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Dry/Reheated:  I kid you not I walked in after the lunch rush \n",
    "and ordered a few slices of pepperoni pizza, the guy \n",
    "(without washing his hands after taking the money) walks \n",
    "to some left over lunch cheese pizza sitting on a counter tosses \n",
    "some pepperoni on it and chucks it back into the oven.  \n",
    "This pizza looked like it had been out for a few hours and it tasted like it \n",
    "did too (yes I bit into it against my basic instinct...I was hungry!). \n",
    "Needless to say I didn\\'t finish it, the two men at the front counter were\n",
    "not at all friendly in any way and I felt like I was interrupting break-time \n",
    "when I ordered my slice.  \\n\\nI didn\\'t feel \"Italian\" anything from this \n",
    "restaurant setting or menu.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "splitted_sentences = splitter.split(text)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for r in rules:\n",
    "#     print \"%5.3f %5.3f %s\" % (r.support, r.confidence, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = \"the little yellow dog barked at the cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(pos_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar2 = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "pos_sentences2 = ie_preprocess(text)\n",
    "cp = nltk.RegexpParser(grammar2)\n",
    "# for sent in pos_sentences2:\n",
    "#     print cp.parse(sent)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = [s_word.encode('utf-8') for s_word in set(stopwords.words(\"english\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    preprocessed = []\n",
    "    for sent in sentences: \n",
    "        filtered_words = []\n",
    "        sent = sent.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        sentence = nltk.word_tokenize(sent)\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop and word.lower() not in string.punctuation:\n",
    "                filtered_words.append(wordnet_lemmatizer.lemmatize(spell(word)).encode('utf-8'))\n",
    "        preprocessed.append((nltk.pos_tag(filtered_words)))       \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while using stemmers like porter and Snowball words get stemmed where not necessary as shown in below cases:\n",
    "hungry -> hungri,\n",
    "restaurant -> restaur,\n",
    "anything -> anyth,\n",
    "break-time -> break-tim and so on\n",
    "\n",
    "So decided to use a lemmantizer which extracts lemmas using wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"dogs\")\n",
    "\n",
    "result: dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "#     stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word = word.lower()\n",
    "#     word = stemmer.stem_word(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dog'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_sentences = ie_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_sentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = list(itertools.chain.from_iterable(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Dry', 'NNP'), (u'Reheated', 'VBD'), (u':', ':'), (u'I', 'PRP'), (u'kid', 'VBP'), (u'you', 'PRP'), (u'not', 'RB'), (u'I', 'PRP'), (u'walked', 'VBD'), (u'in', 'IN'), (u'after', 'IN'), (u'the', 'DT'), (u'lunch', 'NN'), (u'rush', 'NN'), (u'and', 'CC'), (u'ordered', 'VBD'), (u'a', 'DT'), (u'few', 'JJ'), (u'slices', 'NNS'), (u'of', 'IN'), (u'pepperoni', 'NN'), (u'pizzaz', 'VBP'), (u'the', 'DT'), (u'guy', 'NN'), (u'without', 'IN'), (u'washing', 'VBG'), (u'his', 'PRP$'), (u'hands', 'NNS'), (u'after', 'IN'), (u'taking', 'VBG'), (u'the', 'DT'), (u'money', 'NN'), (u'walks', 'NNS'), (u'to', 'TO'), (u'some', 'DT'), (u'left', 'VBN'), (u'over', 'RP'), (u'lunch', 'JJ'), (u'cheese', 'JJ'), (u'pizza', 'NN'), (u'sitting', 'VBG'), (u'on', 'IN'), (u'a', 'DT'), (u'counter', 'NN'), (u'tosses', 'VBZ'), (u'some', 'DT'), (u'pepperoni', 'NN'), (u'on', 'IN'), (u'it', 'PRP'), (u'and', 'CC'), (u'chucks', 'VBZ'), (u'it', 'PRP'), (u'back', 'RB'), (u'into', 'IN'), (u'the', 'DT'), (u'oven', 'NN'), (u'This', 'DT'), (u'pizza', 'NN'), (u'looked', 'VBD'), (u'like', 'IN'), (u'it', 'PRP'), (u'had', 'VBD'), (u'been', 'VBN'), (u'out', 'RP'), (u'for', 'IN'), (u'a', 'DT'), (u'few', 'JJ'), (u'hours', 'NNS'), (u'and', 'CC'), (u'it', 'PRP'), (u'tasted', 'VBD'), (u'like', 'IN'), (u'it', 'PRP'), (u'did', 'VBD'), (u'too', 'RB'), (u'eyes', 'JJ'), (u'I', 'PRP'), (u'bit', 'VBP'), (u'into', 'IN'), (u'it', 'PRP'), (u'against', 'IN'), (u'my', 'PRP$'), (u'basic', 'JJ'), (u'instinct', 'JJ'), (u'...I', 'NN'), (u'was', 'VBD'), (u'hungry', 'JJ'), (u')', ')'), (u'.', '.')]\n",
      "dry\n",
      "lunch rush\n",
      "slice\n",
      "pepperoni\n",
      "guy\n",
      "hand\n",
      "money walk\n",
      "lunch cheese pizza\n",
      "counter\n",
      "pepperoni\n",
      "oven\n",
      "pizza\n",
      "hour\n",
      "basic instinct ...i\n",
      "[(u'Needless', 'NN'), (u'to', 'TO'), (u'say', 'VB'), (u'I', 'PRP'), (u'didnt', 'VBP'), (u'finish', 'VB'), (u'it', 'PRP'), (u'the', 'DT'), (u'two', 'CD'), (u'men', 'NNS'), (u'at', 'IN'), (u'the', 'DT'), (u'front', 'NN'), (u'counter', 'NN'), (u'were', 'VBD'), (u'not', 'RB'), (u'at', 'IN'), (u'all', 'DT'), (u'friendly', 'JJ'), (u'in', 'IN'), (u'any', 'DT'), (u'way', 'NN'), (u'and', 'CC'), (u'I', 'PRP'), (u'felt', 'VBP'), (u'like', 'IN'), (u'I', 'PRP'), (u'was', 'VBD'), (u'interrupting', 'VBG'), (u'breaktime', 'NN'), (u'when', 'WRB'), (u'I', 'PRP'), (u'ordered', 'VBD'), (u'my', 'PRP$'), (u'slice', 'NN'), (u'I', 'PRP'), (u'didnt', 'VBP'), (u'feel', 'JJ'), (u'Italians', 'NNPS'), (u'anything', 'NN'), (u'from', 'IN'), (u'this', 'DT'), (u'restaurant', 'NN'), (u'setting', 'NN'), (u'or', 'CC'), (u'menu', 'NN')]\n",
      "needle\n",
      "men\n",
      "front counter\n",
      "way\n",
      "breaktime\n",
      "slice\n",
      "feel italian anything\n",
      "restaurant setting\n",
      "menu\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# text = \"\"\"The Buddha, the Godhead, resides quite as comfortably in the circuits of a digital\n",
    "# computer or the gears of a cycle transmission as he does at the top of a mountain\n",
    "# or in the petals of a flower. To think otherwise is to demean the Buddha...which is\n",
    "# to demean oneself.\"\"\"\n",
    "\n",
    "\n",
    "# Used when tokenizing words\n",
    "sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "\n",
    "# sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "#       ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
    "#     | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "#     | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
    "#     | \\.\\.\\.                # ellipsis\n",
    "#     | [][.,;\"'?():-_`]      # these are separate tokens\n",
    "# '''\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "# stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "#Taken from Su Nam Kim Paper...\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "#     word = stemmer.stem_word(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40) and (word.lower() not in stop)\n",
    "    return accepted\n",
    "\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w)]\n",
    "        yield term\n",
    "        \n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    toks = nltk.regexp_tokenize(sent, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(toks) #nltk.tag.pos_tag(toks)\n",
    "    print postoks\n",
    "    tree = chunker.parse(postoks)\n",
    "    terms = get_terms(tree)\n",
    "    for term in terms:\n",
    "        for word in term:\n",
    "            print word,\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'didnt'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"didn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-0.2.0.tar.gz (3.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.6MB 136kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Running setup.py bdist_wheel for autocorrect ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/datascientist/Library/Caches/pip/wheels/b2/1b/a1/e7e6980a801dcb6402363df7aceb4605f0d34acc23afd79f90\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autofocus'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'restaur'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
