{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 (Ganu et al. 2013; Jakob et al. 2009; Wang et al. 2012)\
\
(Chen and Wang 2013; Liu et al. 2013; Wang et al. 2013\
\
\
(Chen and Wang 2013; Liu et al. 2013)\
\
user\'92s contextual preference can be determined\
(Chen and Chen 2014; Hariri et al. 2011; Levi et al. 2012; Li et al. 2010)\
\
Associating aspect opinions with latent factors:\
\
(Wang et al. 2012) - tensor decomposition\
\
(Qiu et al. 2011) - double propagation - semi-supervised - to expand opinion words and extract aspect terms - aspects opinion ratings\
\
The Latent Dirichlet Allocation (LDA) model is applied to cluster the aspect terms into aspects. The user\'92s opinion rating on one aspect is determined via: Number of Positive Opinion Words/Total Number of Opinion Words.\
\
Associating aspect opinions with latent factors. Some studies model the aspect opinions using latent factors. For instance,Wang et al.(2012) establish a 3-dimensional tensor model R to uncover the complex relationships among users, items, and aspects. The tensor model is an extension of matrix factorization (MF), which preserves the data\'92s multi-dimensional nature and determines the latent factors for each dimension. More formally, in (Wang et al. 2012), it is defined with the size I \'d7 J \'d7 (K + 1), where I, J and K denote the numbers of users, items, and aspects, respectively. To obtain the aspects\'92 opinion ratings, they adopt a semi-supervised method called double propagation (Qiu et al. 2011) to expand opinion words and extract aspect terms. The Latent Dirichlet Allocation (LDA) model is applied to cluster the aspect terms into aspects. The user\'92s opinion rating on one aspect is determined via: Number of Positive Opinion Words/Total Number of Opinion Words. Then, a decomposition method, called CP Weighted OPTimization (CP-WOPT) (Acar et al. 2011), is applied to decompose the high-order tensor into a sum of rankone tensors: A (with size I \'d7 R) for users, B (with size J \'d7 R) for items, and C (with size (K + 1) \'d7 R)) for aspects (note that the first entry in C is the overall rating, and the others are the K aspects\'92 opinion ratings), such that ri,j,k =  R r=1 ai,r bj,r ck,r (19) for all i = 1,..., I, j = 1,..., J , and k = 1,..., K + 1, where ri,j,k is the entry of the tensor model R, and R is R\'92s rank. They then consider the CP decomposition as a weighted least squares problem and minimize the following objective function. fW (A, B,C) = 1 2  I i=1  J j=1 K  +1 k=1  wi,j,k ri,j,k \uc0\u8722  R r=1 ai,r bj,r ck,r 2 (20) where W is a non-negative weight tensor (with the same size as R) defined as wi,j,k =  1 if ri,j,k is known 0 if ri,j,k is unknown (21)\
\
\
\
\
a system that associates aspect opinions with latent factors (Wang et al. 2012).\
\
For new users with few ratings, McAuley and Leskovec (2013) and Seroussi et al. (2011) show that models that integrate review topics with latent factors return more accurate recommendations to users than the standard matrix factorization (MF) model that only considers ratings. This finding suggests that reviews can provide additional preference information relative to the s}